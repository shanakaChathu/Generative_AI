{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SimpleDirectoryReader to load our file\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='b0ae87ee-d79f-4933-827e-002ef0a1f578', embedding=None, metadata={'page_label': '1', 'file_name': 'gpt4all.pdf', 'file_path': 'c:\\\\Users\\\\shana\\\\OneDrive\\\\Desktop\\\\Generative_AI\\\\llama_index\\\\data\\\\gpt4all.pdf', 'file_type': 'application/pdf', 'file_size': 8934308, 'creation_date': '2024-05-30', 'last_modified_date': '2024-05-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='GPT4All: An Ecosystem of Open Source Compressed Language Models\\nYuvanesh Anand\\nNomic AI\\nyuvanesh@nomic.aiZach Nussbaum\\nNomic AI\\nzach@nomic.aiAdam Treat\\nNomic AI\\nadam@nomic.aiAaron Miller\\nNomic AI\\naaron@nomic.ai\\nRichard Guo\\nNomic AI\\nrichard@nomic.aiBen Schmidt\\nNomic AI\\nben@nomic.aiGPT4All Community\\nPlanet Earth\\nBrandon Duderstadt∗\\nNomic AI\\nbrandon@nomic.aiAndriy Mulyar∗\\nNomic AI\\nandriy@nomic.ai\\nAbstract\\nLarge language models (LLMs) have recently\\nachieved human-level performance on a range\\nof professional and academic benchmarks. The\\naccessibility of these models has lagged behind\\ntheir performance. State-of-the-art LLMs re-\\nquire costly infrastructure; are only accessible\\nvia rate-limited, geo-locked, and censored web\\ninterfaces; and lack publicly available code and\\ntechnical reports.\\nIn this paper, we tell the story of GPT4All, a\\npopular open source repository that aims to\\ndemocratize access to LLMs. We outline the\\ntechnical details of the original GPT4All model\\nfamily, as well as the evolution of the GPT4All\\nproject from a single model into a fully fledged\\nopen source ecosystem. It is our hope that\\nthis paper acts as both a technical overview of\\nthe original GPT4All models as well as a case\\nstudy on the subsequent growth of the GPT4All\\nopen source ecosystem.\\n1 Introduction\\nOn March 14 2023, OpenAI released GPT-4, a large\\nlanguage model capable of achieving human level per-\\nformance on a variety of professional and academic\\nbenchmarks. Despite the popularity of the release,\\nthe GPT-4 technical report (OpenAI, 2023) contained\\nvirtually no details regarding the architecture, hard-\\nware, training compute, dataset construction, or training\\nmethod used to create the model. Moreover, users could\\nonly access the model through the internet interface at\\nchat.openai.com, which was severely rate limited and\\nunavailable in several locales (e.g. Italy) (BBC News,\\n2023). Additionally, GPT-4 refused to answer a wide\\n∗Shared Senior Authorshipvariety of queries, responding only with the now infa-\\nmous \"As an AI Language Model, I cannot...\" prefix\\n(Vincent, 2023). These transparency and accessibility\\nconcerns spurred several developers to begin creating\\nopen source large language model (LLM) alternatives.\\nSeveral grassroots efforts focused on fine tuning Meta’s\\nopen code LLaMA model (Touvron et al., 2023; McMil-\\nlan, 2023), whose weights were leaked on BitTorrent\\nless than a week prior to the release of GPT-4 (Verge,\\n2023). GPT4All started as one of these variants.\\nIn this paper, we tell the story of GPT4All. We com-\\nment on the technical details of the original GPT4All\\nmodel (Anand et al., 2023), as well as the evolution of\\nGPT4All from a single model to an ecosystem of several\\nmodels. We remark on the impact that the project has\\nhad on the open source community, and discuss future\\ndirections. It is our hope that this paper acts as both a\\ntechnical overview of the original GPT4All models as\\nwell as a case study on the subsequent growth of the\\nGPT4All open source ecosystem.\\n2 The Original GPT4All Model\\n2.1 Data Collection and Curation\\nTo train the original GPT4All model, we collected\\nroughly one million prompt-response pairs using the\\nGPT-3.5-Turbo OpenAI API between March 20, 2023\\nand March 26th, 2023. In particular, we gathered GPT-\\n3.5-Turbo responses to prompts of three publicly avail-\\nable datasets: the unified chip2 subset of LAION OIG,\\na random sub-sample of Stackoverflow Questions, and\\na sub-sample of Bigscience/P3 (Sanh et al., 2021). Fol-\\nlowing the approach in Stanford Alpaca (Taori et al.,\\n2023), an open source LLaMA variant that came just be-\\nfore GPT4All, we focused substantial effort on dataset\\ncuration.\\nThe collected dataset was loaded into Atlas (AI,\\n2023)—a visual interface for exploring and tagging mas-\\nsive unstructured datasets —for data curation. Using At-arXiv:2311.04931v1  [cs.CL]  6 Nov 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)\n",
    "type(documents)\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index from the documents\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query engine for the index\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The piqa value for GPT4All-J v1.1-breezy* model is 65.2.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# query the engine\n",
    "query = \"What is the piqa value for GPT4All-J v1.1-breezy* model ?\"\n",
    "response = query_engine.query(query)\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
